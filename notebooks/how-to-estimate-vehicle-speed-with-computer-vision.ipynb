{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fda1a3bc46aa48d3bb0743a108b037c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26ec9782ae40422a9b76165b102911b0",
              "IPY_MODEL_1698488ce55941368f03709a8c95859a",
              "IPY_MODEL_c3811c520645492d9abe1fa869f9b0b1"
            ],
            "layout": "IPY_MODEL_622600527f5d451fac074d60c1c66bcd"
          }
        },
        "26ec9782ae40422a9b76165b102911b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34ce8675857445c3ae6117fea4cd1d22",
            "placeholder": "​",
            "style": "IPY_MODEL_c72996404f3f4e4aa15c116b33238d1c",
            "value": "100%"
          }
        },
        "1698488ce55941368f03709a8c95859a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4351cbd743684ee9881185c1401b376c",
            "max": 35345757,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b78ff5a54974458cba97e6a151676ac3",
            "value": 35345757
          }
        },
        "c3811c520645492d9abe1fa869f9b0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ca866fc12c1489ca591680ca9ce4545",
            "placeholder": "​",
            "style": "IPY_MODEL_fd349d794cdf4d1d81864c075dfc147a",
            "value": " 33.7M/33.7M [00:00&lt;00:00, 66.2MB/s]"
          }
        },
        "622600527f5d451fac074d60c1c66bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34ce8675857445c3ae6117fea4cd1d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c72996404f3f4e4aa15c116b33238d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4351cbd743684ee9881185c1401b376c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b78ff5a54974458cba97e6a151676ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "#a351fb",
            "description_width": ""
          }
        },
        "8ca866fc12c1489ca591680ca9ce4545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd349d794cdf4d1d81864c075dfc147a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Estimate Vehicle Speed with Computer Vision\n",
        "\n",
        "---\n",
        "\n",
        "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/uWP6UjDeZvY)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/supervision/tree/develop/examples/speed_estimation)\n",
        "\n",
        "## Accompanying Materials\n",
        "\n",
        "We recommend that you follow along in this notebook while watching the YouTube [tutorial](https://youtu.be/uWP6UjDeZvY) on how to estimate vehicle speed with computer vision. You can also find the code below in Python script form as one of the [supervision/examples](https://github.com/roboflow/supervision/tree/develop/examples).\n",
        "\n",
        "[![YouTube Video](https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91)](https://youtu.be/uWP6UjDeZvY)\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Install\n",
        "- Imports\n",
        "- Download Data\n",
        "- Configuration\n",
        "- Source and Target ROIs\n",
        "- Transform Perspective\n",
        "- Process Video"
      ],
      "metadata": {
        "id": "21YnpEIDVdkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "63hkpNyQD5pw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgJklsACD0f3",
        "outputId": "64df3a67-4826-4ae0-96ab-fcc4e7cf6ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 28 19:24:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "P_cttjd_EZN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCZ_qDnLD87m",
        "outputId": "c3119b13-d045-438b-955c-16442be3056d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Tv_COh1SHvIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from supervision.assets import VideoAssets, download_assets\n",
        "from collections import defaultdict, deque"
      ],
      "metadata": {
        "id": "RT2qm1AIHxMf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data"
      ],
      "metadata": {
        "id": "xTUao8lmEwhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** In this notebook we will use one of the Supervision Assets videos. [Here](https://supervision.roboflow.com/assets) you can learn more about it."
      ],
      "metadata": {
        "id": "CAWgpkO-gg4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_assets(VideoAssets.VEHICLES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "fda1a3bc46aa48d3bb0743a108b037c9",
            "26ec9782ae40422a9b76165b102911b0",
            "1698488ce55941368f03709a8c95859a",
            "c3811c520645492d9abe1fa869f9b0b1",
            "622600527f5d451fac074d60c1c66bcd",
            "34ce8675857445c3ae6117fea4cd1d22",
            "c72996404f3f4e4aa15c116b33238d1c",
            "4351cbd743684ee9881185c1401b376c",
            "b78ff5a54974458cba97e6a151676ac3",
            "8ca866fc12c1489ca591680ca9ce4545",
            "fd349d794cdf4d1d81864c075dfc147a"
          ]
        },
        "id": "_gb-C2YmHObf",
        "outputId": "c91a3fa9-2b55-4a2d-ca76-0c8a3ae42cf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vehicles.mp4 assets \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/35345757 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fda1a3bc46aa48d3bb0743a108b037c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vehicles.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "dNgpknBqMdEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_VIDEO_PATH = \"vehicles.mp4\"\n",
        "TARGET_VIDEO_PATH = \"vehicles-result.mp4\"\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.5\n",
        "MODEL_NAME = \"yolov8x.pt\"\n",
        "MODEL_RESOLUTION = 1280"
      ],
      "metadata": {
        "id": "wYfCWNrgIKLo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source and Target ROIs\n",
        "\n",
        "![Source and Target ROIs](https://storage.googleapis.com/com-roboflow-marketing/notebooks/speed-estimation-perspective-1.png)"
      ],
      "metadata": {
        "id": "CV_7BgJiHVpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = np.array([\n",
        "    [1252, 787],\n",
        "    [2298, 803],\n",
        "    [5039, 2159],\n",
        "    [-550, 2159]\n",
        "])\n",
        "\n",
        "TARGET_WIDTH = 25\n",
        "TARGET_HEIGHT = 250\n",
        "\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH - 1, 0],\n",
        "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
        "    [0, TARGET_HEIGHT - 1],\n",
        "])"
      ],
      "metadata": {
        "id": "23BVdBd2HRpn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
        "frame_iterator = iter(frame_generator)\n",
        "frame = next(frame_iterator)"
      ],
      "metadata": {
        "id": "9cByYO3yIH_0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_frame = frame.copy()\n",
        "annotated_frame = sv.draw_polygon(scene=annotated_frame, polygon=SOURCE, color=sv.Color.red(), thickness=4)\n",
        "sv.plot_image(annotated_frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "Ctf9b1gnJReJ",
        "outputId": "06d2fe55-c974-4cdd-f901-1d61216450e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'Color' has no attribute 'red'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-077bb0d4283b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mannotated_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mannotated_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotated_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSOURCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Color' has no attribute 'red'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Perspective"
      ],
      "metadata": {
        "id": "PLKeci5YLcx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViewTransformer:\n",
        "\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        return transformed_points.reshape(-1, 2)"
      ],
      "metadata": {
        "id": "1dU--OuJKkZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)"
      ],
      "metadata": {
        "id": "KtKhXCeFQfc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Video"
      ],
      "metadata": {
        "id": "jOwbeY5USlEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** In this tutorial we use Supervision Annotators. [Here](https://supervision.roboflow.com/annotators) you can learn more about it."
      ],
      "metadata": {
        "id": "4dVPEZdghYxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(MODEL_NAME)\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
        "frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
        "\n",
        "# tracer initiation\n",
        "byte_track = sv.ByteTrack(\n",
        "    frame_rate=video_info.fps, track_thresh=CONFIDENCE_THRESHOLD\n",
        ")\n",
        "\n",
        "# annotators configuration\n",
        "thickness = sv.calculate_dynamic_line_thickness(\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "text_scale = sv.calculate_dynamic_text_scale(\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(\n",
        "    thickness=thickness\n",
        ")\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    text_scale=text_scale,\n",
        "    text_thickness=thickness,\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "trace_annotator = sv.TraceAnnotator(\n",
        "    thickness=thickness,\n",
        "    trace_length=video_info.fps * 2,\n",
        "    position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "\n",
        "polygon_zone = sv.PolygonZone(\n",
        "    polygon=SOURCE,\n",
        "    frame_resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "\n",
        "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\n",
        "\n",
        "# open target video\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "\n",
        "    # loop over source video frame\n",
        "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "        result = model(frame, imgsz=MODEL_RESOLUTION, verbose=False)[0]\n",
        "        detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "        # filter out detections by class and confidence\n",
        "        detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]\n",
        "        detections = detections[detections.class_id != 0]\n",
        "\n",
        "        # filter out detections outside the zone\n",
        "        detections = detections[polygon_zone.trigger(detections)]\n",
        "\n",
        "        # refine detections using non-max suppression\n",
        "        detections = detections.with_nms(IOU_THRESHOLD)\n",
        "\n",
        "        # pass detection through the tracker\n",
        "        detections = byte_track.update_with_detections(detections=detections)\n",
        "\n",
        "        points = detections.get_anchors_coordinates(\n",
        "            anchor=sv.Position.BOTTOM_CENTER\n",
        "        )\n",
        "\n",
        "        # calculate the detections position inside the target RoI\n",
        "        points = view_transformer.transform_points(points=points).astype(int)\n",
        "\n",
        "        # store detections position\n",
        "        for tracker_id, [_, y] in zip(detections.tracker_id, points):\n",
        "            coordinates[tracker_id].append(y)\n",
        "\n",
        "        # format labels\n",
        "        labels = []\n",
        "\n",
        "        for tracker_id in detections.tracker_id:\n",
        "            if len(coordinates[tracker_id]) < video_info.fps / 2:\n",
        "                labels.append(f\"#{tracker_id}\")\n",
        "            else:\n",
        "                # calculate speed\n",
        "                coordinate_start = coordinates[tracker_id][-1]\n",
        "                coordinate_end = coordinates[tracker_id][0]\n",
        "                distance = abs(coordinate_start - coordinate_end)\n",
        "                time = len(coordinates[tracker_id]) / video_info.fps\n",
        "                speed = distance / time * 3.6\n",
        "                labels.append(f\"#{tracker_id} {int(speed)} km/h\")\n",
        "\n",
        "        # annotate frame\n",
        "        annotated_frame = frame.copy()\n",
        "        annotated_frame = trace_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections\n",
        "        )\n",
        "        annotated_frame = bounding_box_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections\n",
        "        )\n",
        "        annotated_frame = label_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections, labels=labels\n",
        "        )\n",
        "\n",
        "        # add frame to target video\n",
        "        sink.write_frame(annotated_frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIyG3oz5RhWO",
        "outputId": "ec61c5f8-22a1-457c-b28d-ed97e59afa86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8x.pt to 'yolov8x.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131M/131M [00:01<00:00, 137MB/s]\n",
            "100%|██████████| 538/538 [02:13<00:00,  4.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏆 Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ],
      "metadata": {
        "id": "U5qtyLNgVRQD"
      }
    }
  ]
}